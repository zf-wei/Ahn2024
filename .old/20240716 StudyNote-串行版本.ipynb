{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c80ab6",
   "metadata": {},
   "source": [
    "稀疏化比例 epsilon = 0.1\n",
    "\n",
    "RAW_SCORE 这个文件中 对于每个mu值（可以看作第零个坐标）\n",
    "\n",
    "第一个坐标是样本编号\n",
    "\n",
    "第二个坐标 0 代表稀疏化的网络 1 代表原网络\n",
    "\n",
    "第三个坐标是方法 ['Euclidean NMI', 'Spherical NMI', 'Euclidean ECS', 'Spherical ECS']\n",
    "\n",
    "RAW_QF 这个文件中，对于每个mu值（可以看作第零个坐标）\n",
    "\n",
    "第一个坐标是样本编号\n",
    "\n",
    "第二个坐标 0 代表稀疏化的网络 1 代表原网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265c7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#隐藏警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c484120e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate an LFR Network and Draw it\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "\n",
    "\n",
    "n = 1000\n",
    "tau1 = 2  # Power-law exponent for the degree distribution\n",
    "tau2 = 1.1  # Power-law exponent for the community size distribution\n",
    "#mu = 0.1  # Mixing parameter\n",
    "avg_deg = 25  # Average Degree\n",
    "max_deg = int(0.1 * n)  # Max Degree\n",
    "min_commu = 60  # Min Community Size\n",
    "max_commu = int(0.1 * n)  # Max Community Size\n",
    "\n",
    "#for mu in np.arange(0.1, 0.11, 0.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8061b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import *\n",
    "import numpy as np\n",
    "    \n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "\n",
    "def to_networkx(self):\n",
    "    if isinstance(self.graph, scipy.sparse.csr.csr_matrix):\n",
    "        return nx.from_scipy_sparse_matrix(self.graph)\n",
    "    else:\n",
    "        return nx.from_numpy_array(self.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d4380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# This function takes the orthogonal part of G_sparse eigenvectors.\n",
    "# Hopefully, we will get a better community detection result from this treatment.\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def compute_orthogonal_components(G, V):\n",
    "    \"\"\"\n",
    "    Compute the component of each column of V orthogonal to the degree sequence vector of a graph.\n",
    "\n",
    "    Parameters:\n",
    "    G (networkx.classes.graph.Graph): The input graph.\n",
    "    V (numpy.ndarray): The input 2D array.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The component of each column of V orthogonal to the degree sequence vector of the graph.\n",
    "    \"\"\"\n",
    "    # get measure and constant vector\n",
    "    mu = np.array([d for n, d in G.degree()])\n",
    "    u = np.ones(G.number_of_nodes())\n",
    "\n",
    "    # \n",
    "    orthogonal_components = np.zeros_like(V)\n",
    "\n",
    "    # do orthogonal for each column\n",
    "    for i in range(V.shape[1]):\n",
    "        v = V[:, i]\n",
    "        orthogonal_component = v - ((v @ (u * mu)) / (u @ (u * mu))) * u\n",
    "        orthogonal_components[:, i] = orthogonal_component  # \n",
    "\n",
    "    return orthogonal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08c1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function for Laplacian Eigenmap using Cupy. The presence of GPU is required.\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import cupy as cp\n",
    "\n",
    "def lap_cupy(graph, dim):\n",
    "    \"\"\"\n",
    "    Compute the Laplacian embedding of a graph using CuPy.\n",
    "\n",
    "    Parameters:\n",
    "    graph (networkx.classes.graph.Graph): The input graph.\n",
    "    dim (int): The dimension of the embedding.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The Laplacian embedding of the graph.\n",
    "    \"\"\"\n",
    "    # Check inputs\n",
    "    assert isinstance(graph, nx.Graph), \"Input graph must be a NetworkX graph.\"\n",
    "    assert isinstance(dim, int) and dim > 0, \"Input dim must be a positive integer.\"\n",
    "    assert dim < graph.number_of_nodes(), \"Input dim must be less than the number of nodes in the graph.\"\n",
    "\n",
    "    # Convert the adjacency matrix of the graph to a CuPy array\n",
    "    A = cp.asarray(nx.adjacency_matrix(graph, nodelist=graph.nodes(), weight='weight').toarray(), dtype=cp.float64)\n",
    "\n",
    "    # Compute L1 normalization along axis 1 (rows)\n",
    "    row_sums = cp.linalg.norm(A, ord=1, axis=1)\n",
    "    A /= row_sums.reshape(-1, 1)\n",
    "\n",
    "    # Compute the eigenvalues and eigenvectors of I_n - A\n",
    "    I_n = cp.eye(graph.number_of_nodes())\n",
    "    w, v = cp.linalg.eigh(I_n - A)\n",
    "\n",
    "    # Sort the eigenvectors by the real part of the eigenvalues\n",
    "    v = v[:, cp.argsort(w.real)]\n",
    "\n",
    "    # Return the embedding\n",
    "    return v[:, 1:(dim+1)].get().real  # Explicitly convert to NumPy array using .get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ff718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KMeans Clustering using Euclidean and Spherical metrics\n",
    "### Using NMI and ECSim for comparison\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import clusim.sim as sim\n",
    "from clusim.clustering import Clustering\n",
    "\n",
    "\n",
    "def euclid_membership(K, points):\n",
    "    euc_kmeans = KMeans(n_clusters=K, n_init=10)\n",
    "    euc_kmeans.fit(points)\n",
    "\n",
    "    evala_euclid_membership = euc_kmeans.labels_\n",
    "    return evala_euclid_membership\n",
    "\n",
    "def cosine_membership(K, points):\n",
    "    normalized_points = normalize(points)\n",
    "    cos_kmeans = KMeans(n_clusters=K, n_init=10)\n",
    "    cos_kmeans.fit(normalized_points)\n",
    "\n",
    "    evala_cosine_membership = cos_kmeans.labels_\n",
    "    return evala_cosine_membership\n",
    "\n",
    "def calculate_score(evala, intr_list, K):\n",
    "# evala is the embedding vectors\n",
    "# intr_list is the intrinsic community strucuture\n",
    "# K is the number of clusters in Kmeans\n",
    "    return_val = [] # 首先准备好返回值 \n",
    "\n",
    "    intr_clus = Clustering({i: [intr_list[i]] for i in range(len(intr_list))})\n",
    "\n",
    "    evala_euclid_membership = euclid_membership(K, evala)\n",
    "\n",
    "    evala_cosine_membership = cosine_membership(K, evala)\n",
    "\n",
    "    ## compare with intrinsic community structure using NMI\n",
    "    return_val.append(normalized_mutual_info_score(evala_euclid_membership, intr_list, average_method='arithmetic'))\n",
    "    return_val.append(normalized_mutual_info_score(evala_cosine_membership, intr_list, average_method='arithmetic'))\n",
    "    \n",
    "    \n",
    "    evala_euclid_clustering = Clustering(elm2clu_dict={i: [evala_euclid_membership[i]] for i in range(len(evala_euclid_membership))})\n",
    "    evala_cosine_clustering = Clustering(elm2clu_dict={i: [evala_cosine_membership[i]] for i in range(len(evala_cosine_membership))})\n",
    "    \n",
    "    ## compare with intrinsic community structure using ECSim\n",
    "    evala_euclid_similarity = sim.element_sim(intr_clus, evala_euclid_clustering, alpha=0.9)\n",
    "    evala_cosine_similarity = sim.element_sim(intr_clus, evala_cosine_clustering, alpha=0.9)\n",
    "    return_val.append(evala_euclid_similarity)\n",
    "    return_val.append(evala_cosine_similarity)\n",
    "    \n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8355d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "step_total  = 10\n",
    "step_size = 0.05\n",
    "MU = np.arange(step_size, step_size*step_total+0.01, step_size)\n",
    "\n",
    "SAMPLE = 50\n",
    "\n",
    "stat = np.zeros((step_total, 3, 4))\n",
    "\n",
    "K = 15\n",
    "\n",
    "RAW_SCORE={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9f25b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     score_sparse \u001b[38;5;241m=\u001b[39m calculate_score(lap_cupy(G_sparse,K),intrinsic_membership, \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(intrinsic_membership)))\n\u001b[1;32m     60\u001b[0m     RAW_SCORE[mu][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m score_sparse\n\u001b[0;32m---> 61\u001b[0m     score_original \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlap_cupy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintrinsic_membership\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintrinsic_membership\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     RAW_SCORE[mu][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m score_original\n\u001b[1;32m     63\u001b[0m stat[\u001b[38;5;28mint\u001b[39m(mu\u001b[38;5;241m/\u001b[39mstep_size)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(RAW_SCORE[mu],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mcalculate_score\u001b[0;34m(evala, intr_list, K)\u001b[0m\n\u001b[1;32m     44\u001b[0m evala_cosine_clustering \u001b[38;5;241m=\u001b[39m Clustering(elm2clu_dict\u001b[38;5;241m=\u001b[39m{i: [evala_cosine_membership[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(evala_cosine_membership))})\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m## compare with intrinsic community structure using ECSim\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m evala_euclid_similarity \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintr_clus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevala_euclid_clustering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m evala_cosine_similarity \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39melement_sim(intr_clus, evala_cosine_clustering, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     49\u001b[0m return_val\u001b[38;5;241m.\u001b[39mappend(evala_euclid_similarity)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clusim/clusimelement.py:60\u001b[0m, in \u001b[0;36melement_sim\u001b[0;34m(clustering1, clustering2, alpha, r, r2, rescale_path_type, ppr_implementation)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melement_sim\u001b[39m(clustering1, clustering2, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, r2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rescale_path_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, ppr_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprpack\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    The element-centric clustering similarity.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    >>> print(sim.element_sim(clustering1, clustering2, alpha=0.9))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     result_tuple \u001b[38;5;241m=\u001b[39m \u001b[43melement_sim_elscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustering1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustering2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mrescale_path_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_path_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mppr_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mppr_implementation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     elementScores, relabeled_elements \u001b[38;5;241m=\u001b[39m result_tuple\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(elementScores)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clusim/clusimelement.py:138\u001b[0m, in \u001b[0;36melement_sim_elscore\u001b[0;34m(clustering1, clustering2, alpha, r, r2, rescale_path_type, relabeled_elements, ppr_implementation)\u001b[0m\n\u001b[1;32m    133\u001b[0m clu_affinity_matrix2 \u001b[38;5;241m=\u001b[39m make_affinity_matrix(clustering2, alpha\u001b[38;5;241m=\u001b[39malpha, r\u001b[38;5;241m=\u001b[39mr2,\n\u001b[1;32m    134\u001b[0m                                             rescale_path_type\u001b[38;5;241m=\u001b[39mrescale_path_type,\n\u001b[1;32m    135\u001b[0m                                             relabeled_elements\u001b[38;5;241m=\u001b[39mrelabeled_elements, ppr_implementation\u001b[38;5;241m=\u001b[39mppr_implementation)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# use the corrected L1 similarity\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m nodeScores \u001b[38;5;241m=\u001b[39m \u001b[43mcL1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclu_affinity_matrix1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclu_affinity_matrix2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodeScores, relabeled_elements\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clusim/clusimelement.py:173\u001b[0m, in \u001b[0;36mcL1\u001b[0;34m(x, y, alpha)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcL1\u001b[39m(x, y, alpha):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    The normalized similarity value based on the L1 probabilty metric\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    corrected for the guaranteed overlap in probability between the two\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m        The 1d numpy array of L1 similarities between the affinity matrices x and y\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for mu in MU:\n",
    "    RAW_SCORE[mu] = np.zeros((SAMPLE, 3, 4))\n",
    "    for i in range(SAMPLE):\n",
    "        G = LFR_benchmark_graph(\n",
    "        n, tau1, tau2, mu, average_degree=avg_deg, max_degree=max_deg, min_community=min_commu, max_community=max_commu,\n",
    "        seed = 10\n",
    "        )\n",
    "\n",
    "        # Remove multi-edges and self-loops from G\n",
    "        G = nx.Graph(G)\n",
    "        selfloop_edges = list(nx.selfloop_edges(G))\n",
    "        G.remove_edges_from(selfloop_edges)\n",
    "\n",
    "        # LFR 图是有内在的社群结构的，每个节点的社群存储在其 community 属性中，是一个 set\n",
    "        # 通过运行循环，按照内在的社群结构给每个节点一个标签 即为其 intrinsic_membership\n",
    "        # 为了方便 intrinsic_membership 一开始是作为一个 dict 存储的，后来将其转化为一个 list\n",
    "        intrinsic_communities = {frozenset(G.nodes[v][\"community\"]) for v in G}\n",
    "        intrinsic_membership = {}\n",
    "        for node in range(G.number_of_nodes()):\n",
    "            for index, inner_set in enumerate(intrinsic_communities):\n",
    "                if node in inner_set:\n",
    "                    intrinsic_membership[node] = index\n",
    "                    break\n",
    "        intrinsic_membership = list(intrinsic_membership.values())\n",
    "\n",
    "        # Get the edge list and edge weights and transform G from NetworkX to Network\n",
    "\n",
    "        # Get edge list as a numpy array\n",
    "        edge_list = list(G.edges())\n",
    "        edge_list = np.array(edge_list)\n",
    "\n",
    "        # Get edge weights as a numpy array\n",
    "        edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "        edge_weights = np.array(edge_weights)\n",
    "\n",
    "        edge_weights = [edge_weights[edge] if edge in edge_weights else 1 for edge in edge_list]\n",
    "\n",
    "        Gn = Network(edge_list, edge_weights)\n",
    "        # Gn is the representation of G in Network format\n",
    "\n",
    "        # Calculate effective resistance\n",
    "        epsilon=0.1\n",
    "        method='spl'\n",
    "        Effective_R = Gn.effR(epsilon, method)\n",
    "\n",
    "        # spectral sparse version of G in Network format\n",
    "        q = 10000\n",
    "        seed = 2024\n",
    "        Gn_Sparse = Gn.spl(q, Effective_R, seed=seed)\n",
    "\n",
    "        # G_sparse is the spectral sparse version of G in NetworkX format    \n",
    "        G_sparse = to_networkx(Gn_Sparse)\n",
    "        # check whether the sparse version is connected or not\n",
    "        # nx.is_connected(G_sparse)\n",
    "\n",
    "        score_sparse_orth = calculate_score(\n",
    "            compute_orthogonal_components(G, lap_cupy(G_sparse,K)),intrinsic_membership, len(np.unique(intrinsic_membership)))\n",
    "        RAW_SCORE[mu][i,0] = score_sparse_orth\n",
    "        score_sparse = calculate_score(lap_cupy(G_sparse,K),intrinsic_membership, len(np.unique(intrinsic_membership)))\n",
    "        RAW_SCORE[mu][i,1] = score_sparse\n",
    "        score_original = calculate_score(lap_cupy(G,K),intrinsic_membership, len(np.unique(intrinsic_membership)))\n",
    "        RAW_SCORE[mu][i,2] = score_original\n",
    "    stat[int(mu/step_size)-1] = np.mean(RAW_SCORE[mu],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 将 RAW_SCORE 存储到文件\n",
    "with open('raw_score.pkl', 'wb') as file:\n",
    "    pickle.dump(RAW_SCORE, file)\n",
    "\n",
    "print(\"RAW_SCORE 已成功存储到 raw_score.pkl 文件中\")\n",
    "\n",
    "#import pickle\n",
    "\n",
    "# 从文件加载 RAW_SCORE\n",
    "#with open('raw_score.pkl', 'rb') as file:\n",
    "#    loaded_raw_score = pickle.load(file)\n",
    "\n",
    "#print(\"RAW_SCORE 已成功从 raw_score.pkl 文件中加载\")\n",
    "#print(loaded_raw_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d9323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=4\n",
    "\n",
    "# 绘制每列的折线图\n",
    "plt.plot(stat[:, 0, x], label='Column 1', color='r')  # 红色曲线\n",
    "plt.plot(stat[:, 1, x], label='Column 2', color='g')  # 绿色曲线\n",
    "plt.plot(stat[:, 2, x], label='Column 3', color='b')  # 蓝色曲线\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 添加标签\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Line Graph for Each Column')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
